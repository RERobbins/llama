{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45b7f4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "822514ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc1d5ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline, logging\n",
    "from awq import AutoAWQForCausalLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1879c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"TheBloke/Llama-2-70B-AWQ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "057726a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae3efcfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3b1be0d164416a9dfe15cae5e0c9b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 17 files:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replacing layers...: 100%|██████████| 80/80 [00:07<00:00, 10.56it/s]\n",
      "Fusing layers...: 100%|██████████| 80/80 [00:00<00:00, 164.28it/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoAWQForCausalLM.from_quantized(\n",
    "    model_name_or_path, \n",
    "    fuse_layers=True,\n",
    "    trust_remote_code=False, \n",
    "    safetensors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af75994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''{prompt}\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bdc8ca8-7b85-49a4-a257-d21ef2236117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n",
      "Output:  <s> Tell me about AI\n",
      "\n",
      "## Introduction\n",
      "\n",
      "In this assignment you are going to tell me about AI. You are going to write a document that explains what AI is, what it can do, and what you think it will be able to do in the future. This is an essay-type document, and you should use the following guidelines:\n",
      "\n",
      "1. Write in English, preferably in American English.\n",
      "2. Use proper grammar.\n",
      "3. Your text should be at least 400 words.\n",
      "4. You should be able to explain AI to someone who knows nothing about it.\n",
      "5. You should not just list facts, you should also include your opinion.\n",
      "6. You can use as many sources as you want, but you should cite them.\n",
      "7. Your document should have a title, and a list of references.\n",
      "8. The document should be written in a text editor, such as Notepad, Atom, or Sublime.\n",
      "9. The document should be saved as a plain text file, such as `my_essay.txt`.\n",
      "\n",
      "### Questions\n",
      "\n",
      "You should answer the following questions in your essay:\n",
      "\n",
      "1. What is AI?\n",
      "2. What is the history of AI?\n",
      "3. What are the different types of AI?\n",
      "4. What is machine learning?\n",
      "5. What is deep learning?\n",
      "6. What are some examples of AI?\n",
      "7. What are some examples of machine learning?\n",
      "8. What are some examples of deep learning?\n",
      "9. What are the limitations of AI?\n",
      "10. What are the ethical considerations of AI?\n",
      "\n",
      "### Examples\n",
      "\n",
      "Here are some examples of what your essay could look like:\n",
      "\n",
      "1. \"AI is the ability of a computer to perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision-making, and translation between languages.\"\n",
      "2. \"The history of AI can be traced back to the early 20th century, when scientists and mathematicians began to explore the possibility of creating machines that could think and learn like humans.\"\n",
      "3. \"There are three different types of AI: narrow AI, general AI, and superintelligent AI.\"\n",
      "4. \"Machine learning is a subset of AI that focuses on the ability of machines to learn from data, without\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "tokens = tokenizer(\n",
    "    prompt_template,\n",
    "    return_tensors='pt'\n",
    ").input_ids.cuda()\n",
    "\n",
    "# Generate output\n",
    "generation_output = model.generate(\n",
    "    tokens,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "print(\"Output: \", tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2f515fa-8064-4848-b38c-7f3b87ae01d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Pipeline:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LlamaAWQForCausalLM' object has no attribute 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/rerobbins/github/llama_sandbox/llama_awq_experiment.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rerobbins/github/llama_sandbox/llama_awq_experiment.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rerobbins/github/llama_sandbox/llama_awq_experiment.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m*** Pipeline:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/rerobbins/github/llama_sandbox/llama_awq_experiment.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m pipe \u001b[39m=\u001b[39m pipeline(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rerobbins/github/llama_sandbox/llama_awq_experiment.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mtext-generation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rerobbins/github/llama_sandbox/llama_awq_experiment.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rerobbins/github/llama_sandbox/llama_awq_experiment.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rerobbins/github/llama_sandbox/llama_awq_experiment.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rerobbins/github/llama_sandbox/llama_awq_experiment.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rerobbins/github/llama_sandbox/llama_awq_experiment.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39m0.7\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rerobbins/github/llama_sandbox/llama_awq_experiment.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     top_p\u001b[39m=\u001b[39;49m\u001b[39m0.95\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rerobbins/github/llama_sandbox/llama_awq_experiment.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     top_k\u001b[39m=\u001b[39;49m\u001b[39m40\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rerobbins/github/llama_sandbox/llama_awq_experiment.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     repetition_penalty\u001b[39m=\u001b[39;49m\u001b[39m1.1\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rerobbins/github/llama_sandbox/llama_awq_experiment.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rerobbins/github/llama_sandbox/llama_awq_experiment.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(pipe(prompt_template)[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mgenerated_text\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/transformers/pipelines/__init__.py:880\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    869\u001b[0m     model_classes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[1;32m    870\u001b[0m     framework, model \u001b[39m=\u001b[39m infer_framework_load_model(\n\u001b[1;32m    871\u001b[0m         model,\n\u001b[1;32m    872\u001b[0m         model_classes\u001b[39m=\u001b[39mmodel_classes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    877\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m    878\u001b[0m     )\n\u001b[0;32m--> 880\u001b[0m model_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mconfig\n\u001b[1;32m    881\u001b[0m hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_commit_hash\n\u001b[1;32m    882\u001b[0m load_tokenizer \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(model_config) \u001b[39min\u001b[39;00m TOKENIZER_MAPPING \u001b[39mor\u001b[39;00m model_config\u001b[39m.\u001b[39mtokenizer_class \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LlamaAWQForCausalLM' object has no attribute 'config'"
     ]
    }
   ],
   "source": [
    "# Inference can also be done using transformers' pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f479ec-2667-4ffe-ba8a-78fccfd040a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
